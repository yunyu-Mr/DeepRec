{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#User: 7499\t#Item: 4698\n",
      "796479 49833\n"
     ]
    }
   ],
   "source": [
    "from data import Dataset\n",
    "\n",
    "data = Dataset(csvfile='interactions.csv', \n",
    "               num_test_users=500, \n",
    "               sample=0.15, \n",
    "               cut_item=100)\n",
    "\n",
    "train, test = data.get_train_test_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sampling and removing some cold items (count < 100) and cold users (ratings < 20), a small dataset was created.\n",
    "* Users: 7499\n",
    "* Items: 4698\n",
    "* Interactions: 846,312\n",
    "\n",
    "500 users were selected for test. After train test split, there are 796,479 interactions in train set and 49,833 interactions in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "**Causal CNN** was applied to capture the sequences features. In order to be causal, **zero-padding** should be applied before convolution. The following figure shows the basic architecture of the network, which only contains one convolution layer. Actually, more convolution layers can be applied. Theoretically, more layers may produce better result, but may cause over-fitting as well. I would try to find out whether deeper CNN is better or not by experiment.\n",
    "\n",
    "The loss function is **BPR loss** with **negative sampling**. Hinge loss or pointwise loss can also be used, but BPR loss works better in this set.\n",
    "\n",
    "<img src=\"img/seq_cnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental study\n",
    "\n",
    "As it is a top-N recommendation, some top-N metrics were used:\n",
    "* Precision@20\n",
    "* Recall@20\n",
    "* NDCG@20\n",
    "\n",
    "First, I compare popularity baseline (POP), matrix factorization with BPR loss (BPR-MF) and sequence-based CNN (Seq-CNN). The following table show the results. We can see that BPR-MF perform only a little bit better than POP. One of the reason is that the BPR-MF model was not well tuning. Seq-CNN perform much better than the POP baseline. That's because the sequence model can capture the changes of users' taste. In other word, it's time-aware.\n",
    "\n",
    "The sequence model can works better if we change the distribution of negative sampling (Seq-CNN-PN). The more popular the item, the more probable it is that the user knows about it. Thus, higher probability should be used to sample the pop items. The probability distribution that I use is $log(c(i)+1)$, where $c(i)$ is the appeal count of item $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Model        | Precision@20 | Recall@20 | NDCG@20 |\n",
    "| ------------- | ------------ | --------- | ------- |\n",
    "| POP           | 3.62%        | 8.15%     | 0.2596  |\n",
    "| BPR-MF        | 3.80%        | 8.55%     | 0.2744  |\n",
    "| Seq-CNN       | 5.34%        | 10.50%    | 0.3798  |\n",
    "| Seq-CNN-PN    | 5.49%        | 11.19%    | 0.3949  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deeper convolution layers?\n",
    "* Wider embedding dimension?\n",
    "* Different activation function, e.g. ReLU, Tanh?\n",
    "* Residual CNN?\n",
    "* Conv2d instead of Conv1d?\n",
    "* RNN (LSTM, GRU) is not work, why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
