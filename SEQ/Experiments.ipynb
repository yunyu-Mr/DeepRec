{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from importlib import reload\n",
    "import layers\n",
    "reload(layers)\n",
    "\n",
    "from data import Dataset, MiniBatcher\n",
    "from loss import bpr_loss, hinge_loss, top1_loss, bpr2_loss\n",
    "from metric import precision_recall\n",
    "from layers import SeqCNN\n",
    "from model import evaluate\n",
    "from sequence_utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ITEM = 4698 + 1\n",
    "MAX_SEQ_LENGTH = 200      # Max sequence length\n",
    "MIN_SEQ_LENGTH = 20       # Min sequence length\n",
    "EMBEDDING_DIM = 32\n",
    "ACTIVATE = 'tanh'\n",
    "LOSS = bpr_loss\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "L2_NORM = 1e-5\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#User: 7499\t#Item: 4698\n",
      "796479 49833\n"
     ]
    }
   ],
   "source": [
    "from data import Dataset\n",
    "\n",
    "data = Dataset(csvfile='interactions.csv', \n",
    "               num_test_users=500, \n",
    "               sample=0.15, \n",
    "               cut_item=100)\n",
    "\n",
    "train, test = data.get_train_test_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.dropna()\n",
    "test_sequences = pad_sequences(test.item_sequence, maxlen=MAX_SEQ_LENGTH)\n",
    "eval_sequences = test.eval_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniform_prob = np.ones(MAX_ITEM)\n",
    "\n",
    "pop_prob = uniform_prob.copy()\n",
    "pop_list = data.train.item.value_counts()\n",
    "pop_prob[pop_list.index] = pop_list.values\n",
    "log_pop_prob = np.log1p(pop_prob)\n",
    "\n",
    "NEGATIVE_PROB = log_pop_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(seed=123):\n",
    "    # Set random state\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    seed = random_state.randint(-10**8, 10**8)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Data\n",
    "    batcher = MiniBatcher(train, MAX_ITEM, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          maxlen=MAX_SEQ_LENGTH,\n",
    "                          minlen=MIN_SEQ_LENGTH,\n",
    "                          sampling_prob=NEGATIVE_PROB,\n",
    "                          random_state=random_state)\n",
    "    # Model\n",
    "    _net = SeqCNN(num_items=MAX_ITEM,\n",
    "                  embedding_dim=EMBEDDING_DIM,\n",
    "                  num_layers=NUM_LAYERS,\n",
    "                  activate=ACTIVATE)\n",
    "    # Optim\n",
    "    optimizer = optim.Adam(_net.parameters(),\n",
    "                            weight_decay=L2_NORM,\n",
    "                            lr=LEARNING_RATE)\n",
    "    # Loss function\n",
    "    loss_function = LOSS\n",
    "\n",
    "    # Iteration\n",
    "    for i in range(1, EPOCHS+1):\n",
    "        _net.train(True)\n",
    "        epoch_loss = 0.0\n",
    "        start = time()\n",
    "\n",
    "        # Batch training\n",
    "        for j, (batch_seq, batch_neg) in enumerate(batcher):  # __iter__\n",
    "            # Input\n",
    "            sequences_var     = Variable(torch.from_numpy(batch_seq.astype('int64')))\n",
    "            neg_sequences_var = Variable(torch.from_numpy(batch_neg.astype('int64')))\n",
    "            mask = sequences_var > 0\n",
    "\n",
    "            # Sequence representations\n",
    "            user_repr, _ = _net.user_representation(sequences_var)\n",
    "\n",
    "            # Score\n",
    "            positive_pred = _net(user_repr, sequences_var)\n",
    "            negative_pred = _net(user_repr, neg_sequences_var)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = loss_function(positive_pred, negative_pred, mask)\n",
    "            epoch_loss += loss.data[0]\n",
    "\n",
    "            # Backward & update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    p,r,DCG = evaluate(_net, test_sequences, eval_sequences, MAX_ITEM)\n",
    "    return p, r, DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def several_training(tag):\n",
    "    \n",
    "    df = pd.DataFrame(columns=['precision%', 'recall%', 'NDCG'])\n",
    "    \n",
    "    for j, seed in enumerate(seeds):\n",
    "        p,r,d = training(seed)\n",
    "        f.write(\"{0}\\t{1:.2f}\\t{2:.2f}\\t{3:.4f}\\n\".format(j, p*100, r*100, d))\n",
    "        df.loc[j] = [p*100, r*100, d]\n",
    "            \n",
    "    df.to_csv(\"log/{}.csv\".format(tag), \n",
    "              index=False, \n",
    "              float_format=\"%g\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling (unifrom vs log_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = {'uniform': uniform_prob, 'log_pop': log_pop_prob}\n",
    "seeds = [1,10,100,200,500]\n",
    "explore_prob = {}\n",
    "\n",
    "with open(\"log/negative_sampling_prob.log\", 'w', 1) as f:\n",
    "    for name, prob in probs.items():\n",
    "        NEGATIVE_PROB = prob\n",
    "        f.write(\"Negative sampling prob: {}\\n\".format(name))\n",
    "\n",
    "        explore_prob[name] = several_training(name)\n",
    "\n",
    "NEGATIVE_PROB = log_pop_prob  # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [1,2,3]\n",
    "activates = ['tanh', 'relu']#, 'hybrid']\n",
    "seeds = [1,10,100,200,500]\n",
    "explore_layer = {}\n",
    "\n",
    "with open(\"log/layer_activation.log\", 'w', 1) as f:  # buffsize=1, flush a line each time.\n",
    "    for layer in layers:\n",
    "        for activate in activates:\n",
    "            # New params\n",
    "            NUM_LAYERS = layer\n",
    "            ACTIVATE = activate\n",
    "            f.write(\"Layer: {}\\tActivate: {}\\n\".format(NUM_LAYERS, ACTIVATE))\n",
    "\n",
    "            # Create dateframe\n",
    "            combine = 'conv{}_{}'.format(layer, activate)\n",
    "            explore_layer[combine] = several_training(combine)\n",
    "# reset\n",
    "NUM_LAYERS = 1\n",
    "ACTIVATE = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, df in explore_layer.items():\n",
    "    print(k, \"Conv layers\")\n",
    "    for col, mean, std in zip(df.columns, df.mean(), df.std()):\n",
    "        print(\"{0}: \\t{1:.4f}+{2:.4f}\".format(col, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "embeddings = [8, 16, 32, 48, 64, 100]\n",
    "seeds = [1,10,100,200,500]\n",
    "explore_emb = {}\n",
    "\n",
    "with open(\"log/embedding_dim.log\", 'w', 1) as f:\n",
    "    for e in embeddings:\n",
    "        EMBEDDING_DIM = e \n",
    "        f.write(\"Eembbeding dim: {}\\n\".format(EMBEDDING_DIM))\n",
    "\n",
    "        # Create dateframe\n",
    "        tag = 'embedding{}'.format(e)\n",
    "        explore_emb[tag] = several_training(tag)\n",
    "EMBEDDING_DIM = 32  # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mean = pd.DataFrame(columns=['precision%', 'recall%', 'NDCG'])\n",
    "df_std = pd.DataFrame(columns=['precision%', 'recall%', 'NDCG'])\n",
    "for k, df in explore_emb.items():\n",
    "    print(k, \"Embedding dim\")\n",
    "    df_mean.loc[k] = df.mean()\n",
    "    df_std.loc[k] = df.std()\n",
    "    for col, mean, std in zip(df.columns, df.mean(), df.std()):\n",
    "        print(\"{0}: \\t{1:.4f}+{2:.4f}\".format(col, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "losses = {'top1_loss': top1_loss, 'bpr2_loss': bpr2_loss}\n",
    "seeds = [1,10,100,200,500]\n",
    "explore_loss = {}\n",
    "\n",
    "with open(\"log/loss_function.log\", 'w', 1) as f:\n",
    "    for name, loss in losses.items():\n",
    "        LOSS = loss\n",
    "        f.write(\"Loss function: {}\\n\".format(name))\n",
    "\n",
    "        # Create dateframe\n",
    "        explore_loss[name] = several_training(name)\n",
    "LOSS = bpr_loss  # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 万元利息\n",
    "投入万元，计算不同利率下 N 年后的利息。\n",
    "$本息 =本金 \\times (1+利率\\%)^{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1年</th>\n",
       "      <th>3年</th>\n",
       "      <th>5年</th>\n",
       "      <th>10年</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.04</th>\n",
       "      <td>400</td>\n",
       "      <td>1248</td>\n",
       "      <td>2166</td>\n",
       "      <td>4802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>500</td>\n",
       "      <td>1576</td>\n",
       "      <td>2762</td>\n",
       "      <td>6288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>600</td>\n",
       "      <td>1910</td>\n",
       "      <td>3382</td>\n",
       "      <td>7908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>1000</td>\n",
       "      <td>3310</td>\n",
       "      <td>6105</td>\n",
       "      <td>15937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>1500</td>\n",
       "      <td>5208</td>\n",
       "      <td>10113</td>\n",
       "      <td>30455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1年    3年     5年    10年\n",
       "0.04   400  1248   2166   4802\n",
       "0.05   500  1576   2762   6288\n",
       "0.06   600  1910   3382   7908\n",
       "0.10  1000  3310   6105  15937\n",
       "0.15  1500  5208  10113  30455"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, r, n):\n",
    "    \"\"\"\" 计算n年后的本息和 \"\"\"\n",
    "    return x * (1+r)**n\n",
    "\n",
    "x = 10000  # 本金一万\n",
    "rates = [0.04, 0.05, 0.06, 0.1, 0.15] # 利率：4% 5% 6% 10% 15%\n",
    "years = [1,3,5,10] # 年\n",
    "\n",
    "interest = pd.DataFrame(index=rates,\n",
    "                        columns=['1年', '3年', '5年', '10年'])\n",
    "for rate in rates:\n",
    "    interest.loc[rate] = [f(x, rate, year)-x for year in years]\n",
    "interest.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
